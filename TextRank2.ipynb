{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "TensorFlow 2.4 on Python 3.8 (CUDA 11.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "TextRank2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oLU5PS6MTRPb",
        "lx9hgo5tTRPd"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChungbinKim/-/blob/master/TextRank2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOU8hPtyTRPJ",
        "outputId": "1154ad2c-0d66-494e-e149-3fde9c64b902"
      },
      "source": [
        "!pip install konlpy\n",
        "import glob\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import datetime \n",
        "import networkx\n",
        "import json\n",
        "from konlpy.tag import Komoran"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 1.4MB/s \n",
            "\u001b[?25hCollecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9sZCP1c_iZy"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_yV71TWTRPM"
      },
      "source": [
        "tagger = Komoran()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rO2fEgTTRPO"
      },
      "source": [
        "### 1. 전처리용 클래스 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpBNZBKSTRPP"
      },
      "source": [
        "#### 1. A.  원문 텍스트 클린징 + 모델에 문장 할당용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMYn31t-TRPP"
      },
      "source": [
        "class RawSentence:\n",
        "    def __init__(self, textIter):\n",
        "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
        "        else: self.textIter = textIter\n",
        "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
        " \n",
        "    def __iter__(self):\n",
        "        for line in self.textIter:\n",
        "            ch = self.rgxSplitter.split(line)\n",
        "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
        "                if not s: continue\n",
        "                yield s"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7yEgibtTRPQ"
      },
      "source": [
        "class RawSentenceReader:\n",
        "    def __init__(self, filepath):\n",
        "        self.filepath = filepath\n",
        "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
        " \n",
        "    def __iter__(self):\n",
        "        for line in open(self.filepath, encoding='utf-8'):\n",
        "            ch = self.rgxSplitter.split(line)\n",
        "            for s in map(lambda a, b: a + b, ch[::2], ch[1::2]):\n",
        "                if not s: continue\n",
        "                yield s"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrqZfXK4TRPR"
      },
      "source": [
        "#### 1. B. Komoran 형태소 분석기를 활용하여 단어의 품사를 태깅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOFdTkiJTRPS"
      },
      "source": [
        "class RawTagger:\n",
        "    def __init__(self, textIter, tagger = None):\n",
        "        if tagger:\n",
        "            self.tagger = tagger\n",
        "        else :\n",
        "            from konlpy.tag import Komoran\n",
        "            self.tagger = Komoran()\n",
        "        if type(textIter) == str: self.textIter = textIter.split('\\n')\n",
        "        else: self.textIter = textIter\n",
        "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
        " \n",
        "    def __iter__(self):\n",
        "        for line in self.textIter:\n",
        "            ch = self.rgxSplitter.split(line)\n",
        "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
        "                if not s: continue\n",
        "                yield self.tagger.pos(s)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZQMi9nbTRPT"
      },
      "source": [
        "class RawTaggerReader:\n",
        "    def __init__(self, filepath, tagger = None):\n",
        "        if tagger:\n",
        "            self.tagger = tagger\n",
        "        else :\n",
        "            from konlpy.tag import Komoran\n",
        "            self.tagger = Komoran()\n",
        "        self.filepath = filepath\n",
        "        self.rgxSplitter = re.compile('([.!?:](?:[\"\\']|(?![0-9])))')\n",
        " \n",
        "    def __iter__(self):\n",
        "        for line in open(self.filepath, encoding='utf-8'):\n",
        "            ch = self.rgxSplitter.split(line)\n",
        "            for s in map(lambda a,b:a+b, ch[::2], ch[1::2]):\n",
        "                if not s: continue\n",
        "                yield self.tagger.pos(s)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbXLXFHqTRPU"
      },
      "source": [
        "### 2. TextRank 클래스 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oZggg78TRPU"
      },
      "source": [
        "class TextRank:\n",
        "    def __init__(self, **kargs):\n",
        "        self.graph = None\n",
        "        self.window = kargs.get('window', 4)\n",
        "        self.coef = kargs.get('coef', 1.0)\n",
        "        self.threshold = kargs.get('threshold', 0.003)\n",
        "        self.dictCount = {}\n",
        "        self.dictBiCount = {}\n",
        "        self.dictNear = {}\n",
        "        self.nTotal = 0\n",
        "        \n",
        "    def load(self, sentenceIter, wordFilter = None):\n",
        "        def insertPair(a, b):\n",
        "            if a > b: a, b = b, a\n",
        "            elif a == b: return\n",
        "            self.dictBiCount[a, b] = self.dictBiCount.get((a, b), 0) + 1\n",
        " \n",
        "        def insertNearPair(a, b):\n",
        "            self.dictNear[a, b] = self.dictNear.get((a, b), 0) + 1\n",
        " \n",
        "        for sent in sentenceIter:\n",
        "            for i, word in enumerate(sent):\n",
        "                if wordFilter and not wordFilter(word): continue\n",
        "                self.dictCount[word] = self.dictCount.get(word, 0) + 1\n",
        "                self.nTotal += 1\n",
        "                if i - 1 >= 0 and (not wordFilter or wordFilter(sent[i-1])): insertNearPair(sent[i-1], word)\n",
        "                if i + 1 < len(sent) and (not wordFilter or wordFilter(sent[i+1])): insertNearPair(word, sent[i+1])\n",
        "                for j in range(i+1, min(i+self.window+1, len(sent))):\n",
        "                    if wordFilter and not wordFilter(sent[j]): continue\n",
        "                    if sent[j] != word: insertPair(word, sent[j])\n",
        " \n",
        "    def loadSents(self, sentenceIter, tokenizer = None):\n",
        "        import math\n",
        "        def similarity(a, b):\n",
        "            n = len(a.intersection(b))\n",
        "            return n / float(len(a) + len(b) - n) / (math.log(len(a)+1) * math.log(len(b)+1))\n",
        " \n",
        "        if not tokenizer: rgxSplitter = re.compile('[\\\\s.,:;-?!()\"\\']+')\n",
        "        sentSet = []\n",
        "        for sent in filter(None, sentenceIter):\n",
        "            if type(sent) == str:\n",
        "                if tokenizer: s = set(filter(None, tokenizer(sent)))\n",
        "                else: s = set(filter(None, rgxSplitter.split(sent)))\n",
        "            else: s = set(sent)\n",
        "            if len(s) < 2: continue\n",
        "            self.dictCount[len(self.dictCount)] = sent\n",
        "            sentSet.append(s)\n",
        " \n",
        "        for i in range(len(self.dictCount)):\n",
        "            for j in range(i+1, len(self.dictCount)):\n",
        "                s = similarity(sentSet[i], sentSet[j])\n",
        "                if s < self.threshold: continue\n",
        "                self.dictBiCount[i, j] = s\n",
        " \n",
        "    def getPMI(self, a, b):\n",
        "        import math\n",
        "        co = self.dictNear.get((a, b), 0)\n",
        "        if not co: return None\n",
        "        return math.log(float(co) * self.nTotal / self.dictCount[a] / self.dictCount[b])\n",
        " \n",
        "    def getI(self, a):\n",
        "        import math\n",
        "        if a not in self.dictCount: return None\n",
        "        return math.log(self.nTotal / self.dictCount[a])\n",
        " \n",
        "    def build(self):\n",
        "        self.graph = networkx.Graph()\n",
        "        self.graph.add_nodes_from(self.dictCount.keys())\n",
        "        for (a, b), n in self.dictBiCount.items():\n",
        "            self.graph.add_edge(a, b, weight=n*self.coef + (1-self.coef))\n",
        " \n",
        "    def rank(self):\n",
        "        return networkx.pagerank(self.graph, weight='weight')\n",
        " \n",
        "    def extract(self, ratio = 0.1):\n",
        "        ranks = self.rank()\n",
        "        cand = sorted(ranks, key=ranks.get, reverse=True)[:int(len(ranks) * ratio)]\n",
        "        pairness = {}\n",
        "        startOf = {}\n",
        "        tuples = {}\n",
        "        for k in cand:\n",
        "            tuples[(k,)] = self.getI(k) * ranks[k]\n",
        "            for l in cand:\n",
        "                if k == l: continue\n",
        "                pmi = self.getPMI(k, l)\n",
        "                if pmi: pairness[k, l] = pmi\n",
        " \n",
        "        for (k, l) in sorted(pairness, key=pairness.get, reverse=True):\n",
        "            print(k[0], l[0], pairness[k, l])\n",
        "            if k not in startOf: startOf[k] = (k, l)\n",
        " \n",
        "        for (k, l), v in pairness.items():\n",
        "            pmis = v\n",
        "            rs = ranks[k] * ranks[l]\n",
        "            path = (k, l)\n",
        "            tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
        "            last = l\n",
        "            while last in startOf and len(path) < 7:\n",
        "                if last in path: break\n",
        "                pmis += pairness[startOf[last]]\n",
        "                last = startOf[last][1]\n",
        "                rs *= ranks[last]\n",
        "                path += (last,)\n",
        "                tuples[path] = pmis / (len(path) - 1) * rs ** (1 / len(path)) * len(path)\n",
        " \n",
        "        used = set()\n",
        "        both = {}\n",
        "        for k in sorted(tuples, key=tuples.get, reverse=True):\n",
        "            if used.intersection(set(k)): continue\n",
        "            both[k] = tuples[k]\n",
        "            for w in k: used.add(w)\n",
        " \n",
        "        #for k in cand:\n",
        "        #    if k not in used or True: both[k] = ranks[k] * self.getI(k)\n",
        " \n",
        "        return both\n",
        " \n",
        "    def summarize(self, ratio = 0.4):\n",
        "        r = self.rank()\n",
        "        ks = sorted(r, key=r.get, reverse=True)[:int(len(r)*ratio)]\n",
        "        return ' '.join(map(lambda k:self.dictCount[k], sorted(ks)))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zl9uUtoeTRPY"
      },
      "source": [
        "### 3. TextRank 알고리즘을 활용하여 문서를 요약하는 함수 정의\n",
        "\n",
        "- (참고) 요약 비율은 문서의 20%\n",
        "- (참고) \"휴면\"이 포함된 문장은 모두 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p-F2HZ1Wi_F"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcThWw8OTRPZ"
      },
      "source": [
        "def summarise_contents(x):\n",
        "    tr = TextRank()\n",
        "\n",
        "    stopword = set([('있', 'VV'), ('하', 'VV'), ('되', 'VV'),('없','VV') ])\n",
        "    #텍스트랭크 계산에는 들어가지 않지만 출력값에는 들어도 되는 단어\n",
        "  \n",
        "    \n",
        "    tr.loadSents(RawSentence(x), lambda sent: filter(lambda y:y not in stopword and y[1] in ('NNG', 'NNP', 'VV', 'VA'), tagger.pos(sent)))\n",
        "    tr.build()\n",
        "    ranks = tr.rank()\n",
        "    if len(x)>15000:\n",
        "      a= tr.summarize(0.05)\n",
        "    elif len(x)>10000:\n",
        "      a= tr.summarize(0.09)\n",
        "    else:\n",
        "      a= tr.summarize(0.15) \n",
        "    return a\n",
        "    #return tr.summarize(0.15)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qC6hl41TRPa"
      },
      "source": [
        "def extract_resting(x): #여기서 꼭 들어가야 되는 단어 정하기\n",
        "    \n",
        "    out = ''\n",
        "    for line in x.split('\\n'):\n",
        "        if not line.isspace() and '휴면' in line:\n",
        "            out += line + ' '\n",
        "    return out.strip()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oB1In9QZIHGu"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CJrF5SNjQ6K"
      },
      "source": [
        "def extract_resting2(x): #여기서 들어가면 안되는 단어 정하기\n",
        "  \n",
        "    out = ''\n",
        "    for line in x.split('\\n'): #텍스트랭크 계산에도 들어가지 않으며 출력결과에도 들어가지 않음\n",
        "        if not line.isspace() and '준수합니다' not in line and '노력합니다' not in line and '항에' not in line and '각 호' not in line and '말합니다' not in line and '의미합니다' not in line and '각호' not in line and '경우' not in line:\n",
        "            out += line + ' '\n",
        "    return out.strip()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16EyQLb6jRg4"
      },
      "source": [
        "def extract_resting3(x): #부칙도 필요하려나\n",
        "    \n",
        "    out = ''\n",
        "    for line in x.split('\\n'):\n",
        "        if not line.isspace() and '부칙' in line:\n",
        "            out += line + ' '\n",
        "    return out.strip()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrXnY6f_TRPa"
      },
      "source": [
        "### 4. Data load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLU5PS6MTRPb"
      },
      "source": [
        "#### 4. A. Data load _ Colab ver."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTTXwzNLTRPb"
      },
      "source": [
        "#### Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-jBAJgwTRPc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf4de381-43df-421c-b9f4-1cd8d60ea430"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb6W4vZlTRPc"
      },
      "source": [
        "file_list = glob.glob('./drive/MyDrive/data/*_raw.txt')\n",
        "regex_idx = re.compile('[0-9]+')\n",
        "regex_text = re.compile('[^0-9a-zA-Z가-힣.()\\n]+')\n",
        "dict_ = {}\n",
        "\n",
        "for file in file_list:\n",
        "    sentence = ''\n",
        "    idx = re.findall(regex_idx, file)[0]\n",
        "    with open(file, 'r') as f:\n",
        "        for line in f.readlines():\n",
        "            sentence += line\n",
        "    dict_[int(idx)] = re.sub(regex_text, ' ', sentence)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx9hgo5tTRPd"
      },
      "source": [
        "#### 4. B. Data load _ Local ver."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD82V7JzTRPd"
      },
      "source": [
        "#### Local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lh_esThMTRPe"
      },
      "source": [
        "#file_list = glob.glob('../data/*_raw.txt')\n",
        "#regex_idx = re.compile('[0-9]+')\n",
        "#regex_text = re.compile('[^0-9a-zA-Z가-힣.()\\n]+')\n",
        "#dict_ = {}\n",
        "\n",
        "#for file in file_list:\n",
        " #   sentence = ''\n",
        "  #  idx = re.findall(regex_idx, file)[0]\n",
        "   # with open(file, 'r') as f:\n",
        "    #     for line in f.readlines():\n",
        "     #        sentence += line\n",
        "    #dict_[int(idx)] = re.sub(regex_text, ' ', sentence)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1xRT48JV6La"
      },
      "source": [
        "#glob.glob('../data/*_raw.txt')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZuqEDppWbrT"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Cl6tGYrTRPe"
      },
      "source": [
        "data = pd.DataFrame.from_dict(dict_, orient='index', columns=['Text'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnajZAanTRPf"
      },
      "source": [
        "### 5. 문서요약 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RC-gB1i3TRPf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20625d26-bb4f-4a29-e499-7671fe0663fd"
      },
      "source": [
        "data['Text2'] = data['Text'].map(lambda x : extract_resting2(x))\n",
        "\n",
        "data['Text_Rank'] = data['Text2'].map(lambda x: (summarise_contents(x)) if len(x) >1000 else \"The sentence is too short to summarize..\")\n",
        "data['Resting'] = data['Text'].map(lambda x :  extract_resting(x))\n",
        "data['Resting2'] = data['Text'].map(lambda x :  extract_resting3(x))\n",
        "data['Summary'] = data['Text_Rank'] + data['Resting2']\n",
        "#data['Summary'] = data['Resting2']\n",
        "\n",
        "# df.to_csv('sample.csv', index = False) # 결과를 저장\n",
        "print(\"Done ! \")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done ! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeEcs20imHpF"
      },
      "source": [
        "#k = input('몇번 문서를 요약하겠습니까?, 숫자만 쓸것')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pkt47aA9ms66"
      },
      "source": [
        "#i = int(k)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpl4EJsSTRPg"
      },
      "source": [
        "#data['Text'][i] # Dataframe의 index는 sample 번호와 일치 ,원문"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGyv9vRCMo88"
      },
      "source": [
        "#data['Text2'][i] #원문에서 stopword가 들어간 문장 전체를 제거한 것"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZ25kX3ITRPg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b2b6a876-3444-407f-94f9-981a8400b614"
      },
      "source": [
        "data['Summary'][1]#요약문 \n",
        "#a=data['Summary'][1]# 만약 data['Summary'][1]값이 출력 안될때 \n",
        "#print(a)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'서비스 이용약관   제1조 목적 본 약관(이하 약관 )은 웰(이하 회사)와 회원에 관한 제반사항을 규정함을 목적으로 합니다.   제9조 회원에 대한 통지 제10조 회사의 의무  회사는 본 약관이 정하는 바에 따라 지속적이고 안정적인 서비스를 제공하는데 최선을 다하여야 합니다.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oba41vB3U8Z3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "c49ea737-e297-4696-b645-d8e7ee272940"
      },
      "source": [
        "b=data['Resting'][1]\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "print()\n",
        "bc=b.split()\n",
        "#print(bc)\n",
        "list = []\n",
        "for word in bc:\n",
        "    if word.endswith('년'):\n",
        "        list.append(word)\n",
        "        break\n",
        "    elif word.endswith('년이상'):\n",
        "        list.append(word[0:-2])\n",
        "        break     \n",
        "    elif word.endswith('개월'):\n",
        "        list.append(word)\n",
        "        break\n",
        "    elif word.endswith('개월이상'):\n",
        "        list.append(word[0:-2])\n",
        "        break    \n",
        "if '삭제' in b:\n",
        "  dell = 'O'\n",
        "else:\n",
        "  dell = 'X'  \n",
        "if '메일' in b:\n",
        "  ddell = 'O'\n",
        "else:\n",
        "  ddell = 'X' \n",
        "#print('    휴면기간       :' , list)\n",
        "#print('장기휴면시 계정 삭제:',dell)\n",
        "#print('휴면계정 이메일 통보:',ddell)\n",
        "\n",
        "col = ['서비스']  # columns 리스트\n",
        "ind = ['휴면기간', '장기휴면시 계정 삭제', '휴면계정 이메일 통보' ]        # index 리스트\n",
        "con = [list,dell,ddell]\n",
        "pd.DataFrame(con, columns=col, index=ind)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>서비스</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>휴면기간</th>\n",
              "      <td>24개월</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>장기휴면시 계정 삭제</th>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>휴면계정 이메일 통보</th>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              서비스\n",
              "휴면기간         24개월\n",
              "장기휴면시 계정 삭제     O\n",
              "휴면계정 이메일 통보     O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    }
  ]
}